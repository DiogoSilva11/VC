{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from google.colab import drive\n",
    "from xml.etree import ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount drive on colab notebook\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip data files\n",
    "\n",
    "!unzip \"/content/drive/MyDrive/02 - tagged1.zip\" -d \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# major variables\n",
    "\n",
    "photos_dir = '/content/photos'\n",
    "renders_dir = '/content/renders'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(xml_file):\n",
    "    '''\n",
    "    Read the xml file and return the bounding box coordinates\n",
    "    '''\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    bounding_boxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "        bounding_boxes.append([xmin, ymin, xmax, ymax])\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    '''\n",
    "    Returns a list of images and labels for each image\n",
    "    '''\n",
    "    image_paths = []\n",
    "    num_legos = []\n",
    "    for subdir, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                n = int(subdir.split(os.sep)[-1])\n",
    "                image_paths.append(os.path.join(subdir, file))\n",
    "                num_legos.append(n)\n",
    "    combined = list(zip(image_paths, num_legos))\n",
    "    combined.sort()\n",
    "    image_paths, num_legos = zip(*combined)\n",
    "    image_paths = np.asarray(image_paths)\n",
    "    num_legos = torch.Tensor(num_legos).to(torch.int64)\n",
    "    return image_paths, num_legos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "image_paths, num_legos = load_data(photos_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with defined train test split\n",
    "\n",
    "train_test_split = np.genfromtxt('/content/drive/MyDrive/train_test_split.csv', delimiter=',', dtype=None, encoding=None)\n",
    "\n",
    "train_test_ids = {\n",
    "    'train': [],\n",
    "    'test': []\n",
    "}\n",
    "for index, row in enumerate(train_test_split):\n",
    "    if row[1] == '1':\n",
    "      train_test_ids['test'].append(index - 1)\n",
    "    elif row[1] == '0':\n",
    "      train_test_ids['train'].append(index - 1)\n",
    "\n",
    "len(train_test_ids['train']), len(train_test_ids['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "\n",
    "indices = train_test_ids['test']\n",
    "np.random.shuffle(indices, )\n",
    "\n",
    "test_size = 0.4 * len(indices)\n",
    "split = int(np.floor(test_size))\n",
    "train_test_ids['valid'], train_test_ids['test'] = indices[split:], indices[:split]\n",
    "\n",
    "len(train_test_ids['train']), len(train_test_ids['valid']), len(train_test_ids['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distribution in training data\n",
    "\n",
    "num_legos_train = num_legos[train_test_ids['train']]\n",
    "plt.hist(num_legos_train, bins=range(1, max(num_legos_train)), align='left', rwidth=0.8)\n",
    "plt.xlabel('Number of Legos')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of Legos Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersampling of larger class in training data\n",
    "\n",
    "indices = []\n",
    "for i in train_test_ids['train']:\n",
    "    if num_legos[i] == 1:\n",
    "        indices.append(i)\n",
    "np.random.shuffle(indices, )\n",
    "leftovers_size = 0.8 * len(indices)\n",
    "split = int(np.floor(leftovers_size))\n",
    "_, leftovers = indices[split:], indices[:split]\n",
    "for i in leftovers:\n",
    "    train_test_ids['train'].remove(i)\n",
    "\n",
    "num_legos_train = num_legos[train_test_ids['train']]\n",
    "plt.hist(num_legos_train, bins=range(1, max(num_legos_train)), align='left', rwidth=0.8)\n",
    "plt.xlabel('Number of Legos')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of Legos Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegosDataset(Dataset):\n",
    "    '''\n",
    "    Dataset class for the legos dataset\n",
    "    '''\n",
    "    def __init__(self, images_filenames, num_legos, transform=None):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.transform = transform\n",
    "        self.labels = num_legos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = cv2.imread(image_filename)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, valid and test datasets\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = LegosDataset(image_paths[train_test_ids['train']], num_legos[train_test_ids['train']], transform=transform)\n",
    "valid_dataset = LegosDataset(image_paths[train_test_ids['valid']], num_legos[train_test_ids['valid']], transform=transform)\n",
    "test_dataset = LegosDataset(image_paths[train_test_ids['test']], num_legos[train_test_ids['test']], transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cpu or gpu device for training\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change definition\n",
    "\n",
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    '''\n",
    "    CNN for a regression task\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put model in device\n",
    "\n",
    "model = ConvolutionalNeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_iter(dataloader, model, loss_fn, optimizer=None, is_train=True):\n",
    "    '''\n",
    "    Function for one epoch iteration\n",
    "    '''\n",
    "    if is_train:\n",
    "        assert optimizer is not None, \"When training, please provide an optimizer\"\n",
    "    num_batches = len(dataloader)\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss = 0.0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "            X, y = X.float().to(device), y.float().to(device)\n",
    "            pred = model(X).squeeze()\n",
    "            loss = loss_fn(pred, y)\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            preds.extend(pred.view(-1).cpu().detach().numpy())\n",
    "            labels.extend(y.view(-1).cpu().numpy())\n",
    "    return total_loss / num_batches, np.mean((np.array(labels) - np.array(preds))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer):\n",
    "    '''\n",
    "    Function for training the model\n",
    "    '''\n",
    "    train_history = {'loss': [], 'accuracy': []}\n",
    "    val_history = {'loss': [], 'accuracy': []}\n",
    "    best_val_loss = np.inf\n",
    "    print(\"Start training...\")\n",
    "\n",
    "    for t in range(num_epochs):\n",
    "        print(f\"Epoch {t+1}/{num_epochs}\")\n",
    "        train_loss, train_acc = epoch_iter(train_dataloader, model, loss_fn, optimizer)\n",
    "        print(f\"Train loss: {train_loss:.3f}, Train accuracy: {train_acc:.3f}\")\n",
    "        val_loss, val_acc = epoch_iter(validation_dataloader, model, loss_fn, is_train=False)\n",
    "        print(f\"Validation loss: {val_loss:.3f}, Validation accuracy: {val_acc:.3f}\")\n",
    "\n",
    "        # save model when val loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_dict = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': t\n",
    "            }\n",
    "            torch.save(save_dict, model_name + '_best_model.pth')\n",
    "\n",
    "        # save latest model\n",
    "        save_dict = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': t\n",
    "        }\n",
    "        torch.save(save_dict, model_name + '_latest_model.pth')\n",
    "\n",
    "        # save training history\n",
    "        train_history['loss'].append(train_loss)\n",
    "        train_history['accuracy'].append(train_acc)\n",
    "        val_history['loss'].append(val_loss)\n",
    "        val_history['accuracy'].append(val_acc)\n",
    "\n",
    "    print(\"Finished\")\n",
    "    return train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) # TODO: change optimizer (regression task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "num_epochs = 3 # TODO: change number of epochs to 50 or so (low value is for testing)\n",
    "\n",
    "train_history, val_history = train(model, 'lego_counter', num_epochs, train_dataloader, valid_dataloader, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training evolution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingHistory(train_history, val_history):\n",
    "    '''\n",
    "    Plot the training history of the model\n",
    "    '''\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(train_history['loss'], label='train')\n",
    "    plt.plot(val_history['loss'], label='val')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(train_history['accuracy'], label='train')\n",
    "    plt.plot(val_history['accuracy'], label='val')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training history\n",
    "\n",
    "plotTrainingHistory(train_history, val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "\n",
    "model = ConvolutionalNeuralNetwork().to(device)\n",
    "checkpoint = torch.load('lego_counter_best_model.pth')\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test data\n",
    "\n",
    "test_loss, test_acc = epoch_iter(test_dataloader, model, loss_fn, is_train=False)\n",
    "print(f\"Test loss: {test_loss:.3f}, Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model, dataloader):\n",
    "    '''\n",
    "    Display images along with their true and predicted labels\n",
    "    '''\n",
    "    model.eval() \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_images = []\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in dataloader:  \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images) \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_images.extend(images.cpu().numpy())\n",
    "    for i in range(len(all_images)):\n",
    "        plt.imshow(all_images[i].transpose((1, 2, 0)))\n",
    "        plt.title(f'True label: {all_labels[i]}, Predicted label: {all_preds[i]}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view predictions\n",
    "\n",
    "show_predictions(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
