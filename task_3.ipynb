{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMckmETGa470"
      },
      "source": [
        "# Computer Vision - Task 3 (T1G2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkE8-9cYa474"
      },
      "source": [
        "If you are running this notebook in your local machine:\n",
        "1. create a new empty folder called content\n",
        "2. unzip '02 - tagged1.zip'\n",
        "3. move photos and renders folders into content folder\n",
        "4. add 'train_test_split.csv' file into content folder as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x8_KSp-a476"
      },
      "source": [
        "If you are running this notebook in Google Colab:\n",
        "1. add these files to your Google Drive storage\n",
        "    - '02 - tagged1.zip'\n",
        "    - 'train_test_split.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMXfu3SIa476"
      },
      "source": [
        "#### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bmpe6o1a477",
        "outputId": "c74c2168-f0cc-4bb6-e77e-7d68cd5c9507"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from xml.etree import ElementTree as ET\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from tqdm import tqdm\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image\n",
        "\n",
        "%pip install torchmetrics\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "\n",
        "%pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.data.annotator import auto_annotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9LG-Ii2a47-"
      },
      "outputs": [],
      "source": [
        "def in_colab():\n",
        "    '''\n",
        "    Check if the code is running on Google Colab\n",
        "    '''\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfIyHT1ha47-"
      },
      "outputs": [],
      "source": [
        "if in_colab():\n",
        "    from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E1R5NtSa47_"
      },
      "source": [
        "# LEGO Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQasRvwea48B"
      },
      "source": [
        "#### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r0btI4za48C"
      },
      "outputs": [],
      "source": [
        "# possible strategies\n",
        "\n",
        "cut_remaining = 100\n",
        "apply_augmentation = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH1rqGpaa48C",
        "outputId": "3cf41dad-665e-430a-8af2-ee383fd78901"
      },
      "outputs": [],
      "source": [
        "# mount drive on colab notebook\n",
        "\n",
        "if in_colab():\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u09DnQva48C",
        "outputId": "0f8edc2b-c605-4c5c-85a6-614d0286fdd1"
      },
      "outputs": [],
      "source": [
        "# unzip data files\n",
        "\n",
        "if in_colab():\n",
        "    !unzip \"/content/drive/MyDrive/02 - tagged1.zip\" -d \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR8mtI2ra48D"
      },
      "outputs": [],
      "source": [
        "# data directories\n",
        "\n",
        "photos_dir = '/content/photos' if in_colab() else 'content/photos'\n",
        "renders_dir = '/content/renders' if in_colab() else 'content/renders'\n",
        "split_path = '/content/drive/MyDrive/train_test_split.csv' if in_colab() else 'content/train_test_split.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiTAeVITa48D"
      },
      "outputs": [],
      "source": [
        "def load_data(data_dir):\n",
        "    '''\n",
        "    Returns a list of images and labels for each image\n",
        "    '''\n",
        "    image_paths = []\n",
        "    num_legos = []\n",
        "    for subdir, _, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.jpg'):\n",
        "                n = int(subdir.split(os.sep)[-1])\n",
        "                image_paths.append(os.path.join(subdir, file))\n",
        "                num_legos.append(n)\n",
        "    combined = list(zip(image_paths, num_legos))\n",
        "    combined.sort()\n",
        "    image_paths, num_legos = zip(*combined)\n",
        "    image_paths = np.asarray(image_paths)\n",
        "    num_legos = torch.Tensor(num_legos).to(torch.int64)\n",
        "    return image_paths, num_legos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAs_uClma48D"
      },
      "outputs": [],
      "source": [
        "def parse_xml(xml_file):\n",
        "    '''\n",
        "    Read the xml file and return the bounding box coordinates\n",
        "    '''\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    bounding_boxes = []\n",
        "    for obj in root.findall('object'):\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = int(bbox.find('xmin').text)\n",
        "        ymin = int(bbox.find('ymin').text)\n",
        "        xmax = int(bbox.find('xmax').text)\n",
        "        ymax = int(bbox.find('ymax').text)\n",
        "        bounding_boxes.append([xmin, ymin, xmax, ymax])\n",
        "    return bounding_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYVxvgLda48E"
      },
      "outputs": [],
      "source": [
        "def parse_all_xml(image_paths):\n",
        "    '''\n",
        "    Parse all XML files corresponding to the given image paths.\n",
        "    '''\n",
        "    bounding_boxes = []\n",
        "    for img_path in image_paths:\n",
        "        xml_path = img_path.replace('.jpg', '.xml')\n",
        "        bounding_boxes.append(parse_xml(xml_path))\n",
        "    return bounding_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrKurRhda48E"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "image_paths, num_legos = load_data(photos_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNYEVnXoa48E"
      },
      "outputs": [],
      "source": [
        "# parse bounding boxes for all images\n",
        "\n",
        "bounding_boxes = parse_all_xml(image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "mhF8sO1na48E",
        "outputId": "92283832-f1ac-4a5b-c561-1f6e58fc2ec8"
      },
      "outputs": [],
      "source": [
        "# class distribution in overall data\n",
        "\n",
        "plt.hist(num_legos, bins=range(1, max(num_legos)), align='left', rwidth=0.8)\n",
        "plt.xlabel('Number of LEGOs')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('LEGO Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07BwjL93a48F",
        "outputId": "c938dfcb-e372-4247-a4f6-654183151fdb"
      },
      "outputs": [],
      "source": [
        "# work with defined train test split\n",
        "\n",
        "train_test_split = np.genfromtxt(split_path, delimiter=',', dtype=None, encoding=None)\n",
        "\n",
        "train_test_ids = {\n",
        "    'train': [],\n",
        "    'test': []\n",
        "}\n",
        "for index, row in enumerate(train_test_split):\n",
        "    if row[1] == '1':\n",
        "      train_test_ids['test'].append(index - 1)\n",
        "    elif row[1] == '0':\n",
        "      train_test_ids['train'].append(index - 1)\n",
        "\n",
        "len(train_test_ids['train']), len(train_test_ids['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "5GFhGEYza48F",
        "outputId": "8afcd408-241b-4a58-8540-27c0f34028ff"
      },
      "outputs": [],
      "source": [
        "# class distribution in training data\n",
        "\n",
        "num_legos_train = num_legos[train_test_ids['train']]\n",
        "plt.hist(num_legos_train, bins=range(1, max(num_legos_train)), align='left', rwidth=0.8)\n",
        "plt.xlabel('Number of LEGOs')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('LEGO Training Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DaLmIsua48G"
      },
      "outputs": [],
      "source": [
        "# validation and test sets\n",
        "\n",
        "num_legos_test = num_legos[train_test_ids['test']].numpy()\n",
        "test_ids = train_test_ids['test']\n",
        "test_distribution = Counter(num_legos_test)\n",
        "set1_ids, set2_ids = [], []\n",
        "\n",
        "for cls, count in test_distribution.items():\n",
        "    cls_ids = [test_ids[i] for i in range(len(test_ids)) if num_legos_test[i] == cls]\n",
        "    half_count = count // 2\n",
        "    remainder = count % 2\n",
        "    set1_ids.extend(cls_ids[:half_count])\n",
        "    set2_ids.extend(cls_ids[half_count:half_count*2])\n",
        "    if remainder:\n",
        "        if len(set1_ids) <= len(set2_ids):\n",
        "            set1_ids.append(cls_ids[-1])\n",
        "        else:\n",
        "            set2_ids.append(cls_ids[-1])\n",
        "\n",
        "train_test_ids['test'] = np.array(set1_ids)\n",
        "train_test_ids['valid'] = np.array(set2_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pro2gazsa48G",
        "outputId": "7d1ed5b8-cb0d-41ad-fb8a-73e871dafa3d"
      },
      "outputs": [],
      "source": [
        "# class distribution in validation and test data\n",
        "\n",
        "num_legos_valid = num_legos[train_test_ids['valid']].numpy()\n",
        "num_legos_test = num_legos[train_test_ids['test']].numpy()\n",
        "\n",
        "valid_distribution = Counter(num_legos_valid)\n",
        "test_distribution = Counter(num_legos_test)\n",
        "\n",
        "print(\"Valid length:\", len(train_test_ids['valid']))\n",
        "print(\"Test length:\", len(train_test_ids['test']))\n",
        "\n",
        "print(\"\\nValid class distribution:\")\n",
        "for cls, count in valid_distribution.items():\n",
        "    print(f\"Class {cls}: {count}\")\n",
        "\n",
        "print(\"\\nTest class distribution:\")\n",
        "for cls, count in test_distribution.items():\n",
        "    print(f\"Class {cls}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "hMCaX6Rea48H",
        "outputId": "39776412-2bdf-42a4-8974-477531788b63"
      },
      "outputs": [],
      "source": [
        "# undersampling of larger classes in training data\n",
        "\n",
        "indices1 = []\n",
        "indices2 = []\n",
        "\n",
        "for i in train_test_ids['train']:\n",
        "    if num_legos[i] == 1:\n",
        "        indices1.append(i)\n",
        "    elif num_legos[i] == 2:\n",
        "        indices2.append(i)\n",
        "\n",
        "np.random.shuffle(indices1, )\n",
        "np.random.shuffle(indices2, )\n",
        "\n",
        "leftovers1 = indices1[cut_remaining:]\n",
        "leftovers2 = indices2[cut_remaining:]\n",
        "\n",
        "for i in leftovers1:\n",
        "    train_test_ids['train'].remove(i)\n",
        "for i in leftovers2:\n",
        "    train_test_ids['train'].remove(i)\n",
        "\n",
        "num_legos_train = num_legos[train_test_ids['train']]\n",
        "plt.hist(num_legos_train, bins=range(1, max(num_legos_train)), align='left', rwidth=0.8)\n",
        "plt.xlabel('Number of LEGOs')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('LEGO Training Distribution (Undersampling)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igcvLizTa48H"
      },
      "outputs": [],
      "source": [
        "# bounding boxes for train, valid, and test sets\n",
        "\n",
        "train_boxes = [bounding_boxes[i] for i in train_test_ids['train']]\n",
        "valid_boxes = [bounding_boxes[i] for i in train_test_ids['valid']]\n",
        "test_boxes = [bounding_boxes[i] for i in train_test_ids['test']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiOyKUX3a48I"
      },
      "outputs": [],
      "source": [
        "class LegosDataset(Dataset):\n",
        "    '''\n",
        "    Dataset class for the legos dataset\n",
        "    '''\n",
        "    def __init__(self, images_filenames, num_legos, bounding_boxes, transforms=[], augmented=[]):\n",
        "        self.images_filenames = images_filenames\n",
        "        self.labels = num_legos\n",
        "        self.bounding_boxes = bounding_boxes\n",
        "        self.transforms = transforms\n",
        "        self.augmented = augmented\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_filenames)\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        image_filename = self.images_filenames[id]\n",
        "        label = self.labels[id]\n",
        "        bounding_boxes = self.bounding_boxes[id]\n",
        "        image = cv2.imread(image_filename)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        original_height, original_width = image.shape[:2]\n",
        "        transformation = self.transforms[self.augmented[id]]\n",
        "        image = transformation(image)\n",
        "        scale_w = 640.0 / original_width\n",
        "        scale_h = 640.0 / original_height\n",
        "        scaled_boxes = []\n",
        "        for box in bounding_boxes:\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            x_min = int(x_min * scale_w)\n",
        "            y_min = int(y_min * scale_h)\n",
        "            x_max = int(x_max * scale_w)\n",
        "            y_max = int(y_max * scale_h)\n",
        "            scaled_boxes.append([x_min, y_min, x_max, y_max])\n",
        "        target = {\n",
        "            'boxes': torch.tensor(scaled_boxes, dtype=torch.float32),\n",
        "            'labels': torch.ones((label,), dtype=torch.int64)\n",
        "        }\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQeJbqy-a48J"
      },
      "outputs": [],
      "source": [
        "# train, valid and test sets\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "augment = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))], p=0.5),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = LegosDataset(image_paths[train_test_ids['train']], num_legos[train_test_ids['train']], train_boxes,\n",
        "                             transforms=[transform, augment], augmented=[0]*len(image_paths[train_test_ids['train']]))\n",
        "\n",
        "valid_dataset = LegosDataset(image_paths[train_test_ids['valid']], num_legos[train_test_ids['valid']], valid_boxes,\n",
        "                             transforms=[transform], augmented=[0]*len(image_paths[train_test_ids['valid']]))\n",
        "\n",
        "test_dataset = LegosDataset(image_paths[train_test_ids['test']], num_legos[train_test_ids['test']], test_boxes,\n",
        "                            transforms=[transform], augmented=[0]*len(image_paths[train_test_ids['test']]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMax6wK6a48K"
      },
      "outputs": [],
      "source": [
        "def generate_data(image_paths, num_legos, bounding_boxes, copies=5):\n",
        "    '''\n",
        "    Generate more data by copying images with more than 6 legos\n",
        "    '''\n",
        "    new_image_paths = []\n",
        "    new_num_legos = []\n",
        "    new_bounding_boxes = []\n",
        "    for id in range(len(image_paths)):\n",
        "        if num_legos[id] >= 6:\n",
        "            for _ in range(copies):\n",
        "                new_image_paths.append(image_paths[id])\n",
        "                new_num_legos.append(num_legos[id])\n",
        "                new_bounding_boxes.append(bounding_boxes[id])\n",
        "    return new_image_paths, new_num_legos, new_bounding_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "U6I28fnJa48L",
        "outputId": "2652ae46-d21a-41c7-d79f-3c75134b1403"
      },
      "outputs": [],
      "source": [
        "# oversampling of smaller classes in training data - augmentation\n",
        "\n",
        "if apply_augmentation:\n",
        "    new_image_paths, new_num_legos, new_bounding_boxes = generate_data(\n",
        "                                                            image_paths[train_test_ids['train']],\n",
        "                                                            num_legos[train_test_ids['train']],\n",
        "                                                            train_boxes\n",
        "                                                        )\n",
        "    for img, lbl, bbox in zip(new_image_paths, new_num_legos, new_bounding_boxes):\n",
        "        train_dataset.images_filenames = np.append(train_dataset.images_filenames, img)\n",
        "        train_dataset.labels = torch.cat((train_dataset.labels, torch.tensor([lbl], dtype=torch.int64)))\n",
        "        train_dataset.bounding_boxes.append(bbox)\n",
        "    train_dataset.augmented.extend([1] * len(new_image_paths))\n",
        "    num_legos_train = train_dataset.labels\n",
        "    plt.hist(num_legos_train, bins=range(1, max(num_legos_train)), align='left', rwidth=0.8)\n",
        "    plt.xlabel('Number of LEGOs')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('LEGO Training Distribution (Oversampling)')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RudiiqVza48L",
        "outputId": "97356f44-5c34-4ee0-e9e0-7a89b7bdb796"
      },
      "outputs": [],
      "source": [
        "# dataloaders\n",
        "\n",
        "batch_size = 8\n",
        "num_workers = 2 if in_colab() else 0\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "len(train_dataset), len(valid_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn4RflX0a48M"
      },
      "source": [
        "### Faster R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7JALBCCa48M"
      },
      "source": [
        "#### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JhspDH_a48M",
        "outputId": "aa259bbc-6a46-41f8-a366-9037ac7020cc"
      },
      "outputs": [],
      "source": [
        "# get cpu or gpu device for training\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpaIQboga48N"
      },
      "outputs": [],
      "source": [
        "# faster r-cnn model\n",
        "\n",
        "faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "in_features = faster_rcnn.roi_heads.box_predictor.cls_score.in_features\n",
        "num_classes = 2\n",
        "faster_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V70U3Hu7a48N"
      },
      "outputs": [],
      "source": [
        "# put model in device\n",
        "\n",
        "model = faster_rcnn.to(device)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pke17-ca48O"
      },
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMSlIWALa48O"
      },
      "outputs": [],
      "source": [
        "# possible strategies\n",
        "\n",
        "train_only_head = True\n",
        "train_head_and_rpn = True\n",
        "train_all = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O5S4C7Ua48O"
      },
      "outputs": [],
      "source": [
        "def apply_nms(predictions, iou_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Apply non-maximum suppression to predictions\n",
        "    \"\"\"\n",
        "    boxes = predictions['boxes']\n",
        "    scores = predictions['scores']\n",
        "    labels = predictions['labels']\n",
        "    keep = torchvision.ops.nms(boxes, scores, iou_threshold=iou_threshold)\n",
        "    boxes = boxes[keep]\n",
        "    scores = scores[keep]\n",
        "    labels = labels[keep]\n",
        "    return boxes, scores, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hAlqwEua48O"
      },
      "outputs": [],
      "source": [
        "def compute_map(preds, targets):\n",
        "    \"\"\"\n",
        "    Calculates the Mean Average Precision (mAP) for object detection predictions\n",
        "    \"\"\"\n",
        "    metric = MeanAveragePrecision(box_format='xyxy', iou_type='bbox')\n",
        "    metric.update(preds=preds, target=targets)\n",
        "    values = metric.compute()\n",
        "    return values['map'].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfVBWAPaa48W"
      },
      "outputs": [],
      "source": [
        "def epoch_iter(dataloader, model, optimizer=None, is_train=True, grad_accum_steps=2):\n",
        "    '''\n",
        "    Perform one epoch of training/validation/testing\n",
        "    '''\n",
        "    if is_train:\n",
        "        assert optimizer is not None, \"When training, please provide an optimizer\"\n",
        "    num_batches = len(dataloader)\n",
        "    model.train() if is_train else model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_loss_classifier = 0.0\n",
        "    total_loss_box_reg = 0.0\n",
        "    total_loss_objectness = 0.0\n",
        "    total_loss_rpn_box_reg = 0.0\n",
        "    total_map = 0.0\n",
        "    with torch.set_grad_enabled(is_train):\n",
        "        for batch, (images, targets) in enumerate(tqdm(dataloader)):\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            if is_train:\n",
        "                loss_data = model(images, targets)\n",
        "                loss_classifier = loss_data['loss_classifier']\n",
        "                total_loss_classifier += loss_classifier.item()\n",
        "                loss_box_reg = loss_data['loss_box_reg']\n",
        "                total_loss_box_reg += loss_box_reg.item()\n",
        "                loss_objectness = loss_data['loss_objectness']\n",
        "                total_loss_objectness += loss_objectness.item()\n",
        "                loss_rpn_box_reg = loss_data['loss_rpn_box_reg']\n",
        "                total_loss_rpn_box_reg += loss_rpn_box_reg.item()\n",
        "                losses = (loss_classifier + loss_box_reg + loss_objectness + loss_rpn_box_reg) / grad_accum_steps\n",
        "                optimizer.zero_grad()\n",
        "                losses.backward()\n",
        "                if (batch + 1) % grad_accum_steps == 0:\n",
        "                    optimizer.step()\n",
        "                total_loss += losses.item() * grad_accum_steps\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    predictions = model(images)\n",
        "                    for preds in predictions:\n",
        "                        preds['boxes'], preds['scores'], preds['labels'] = apply_nms(preds)\n",
        "                    map_score = compute_map(predictions, targets)\n",
        "                    total_map += map_score\n",
        "    avg_loss = total_loss / num_batches if is_train else None\n",
        "    avg_loss_classifier = total_loss_classifier / num_batches if is_train else None\n",
        "    avg_loss_box_reg = total_loss_box_reg / num_batches if is_train else None\n",
        "    avg_loss_objectness = total_loss_objectness / num_batches if is_train else None\n",
        "    avg_loss_rpn_box_reg = total_loss_rpn_box_reg / num_batches if is_train else None\n",
        "    avg_map = total_map / num_batches if not is_train else None\n",
        "    return {\n",
        "        'avg_loss': avg_loss,\n",
        "        'avg_loss_classifier': avg_loss_classifier,\n",
        "        'avg_loss_box_reg': avg_loss_box_reg,\n",
        "        'avg_loss_objectness': avg_loss_objectness,\n",
        "        'avg_loss_rpn_box_reg': avg_loss_rpn_box_reg,\n",
        "        'avg_map': avg_map\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJc9G1hwa48Y"
      },
      "outputs": [],
      "source": [
        "def train(model, model_name, num_epochs, train_dataloader, validation_dataloader, optimizer, train_history=None, val_history=None):\n",
        "    '''\n",
        "    Train the model\n",
        "    '''\n",
        "    if train_history is None:\n",
        "        train_history = {\n",
        "            'loss': [],\n",
        "            'loss_classifier': [],\n",
        "            'loss_box_reg': [],\n",
        "            'loss_objectness': [],\n",
        "            'loss_rpn_box_reg': []\n",
        "        }\n",
        "    if val_history is None:\n",
        "        val_history = {'map': []}\n",
        "    best_val_map = -np.inf\n",
        "    print(\"Start training...\")\n",
        "    for t in range(num_epochs):\n",
        "        print(f\"\\nEpoch {t+1}/{num_epochs}\")\n",
        "        train_metrics = epoch_iter(train_dataloader, model, optimizer, is_train=True)\n",
        "        train_loss = train_metrics['avg_loss']\n",
        "        train_loss_classifier = train_metrics['avg_loss_classifier']\n",
        "        train_loss_box_reg = train_metrics['avg_loss_box_reg']\n",
        "        train_loss_objectness = train_metrics['avg_loss_objectness']\n",
        "        train_loss_rpn_box_reg = train_metrics['avg_loss_rpn_box_reg']\n",
        "        print(f\"Train loss: {train_loss:.3f}\")\n",
        "        print(f\"  Classifier loss: {train_loss_classifier:.3f}\")\n",
        "        print(f\"  Box regression loss: {train_loss_box_reg:.3f}\")\n",
        "        print(f\"  Objectness loss: {train_loss_objectness:.3f}\")\n",
        "        print(f\"  RPN box regression loss: {train_loss_rpn_box_reg:.3f}\")\n",
        "        val_metrics = epoch_iter(validation_dataloader, model, is_train=False)\n",
        "        val_map = val_metrics['avg_map']\n",
        "        print(f\"Validation mAP: {val_map:.3f}\")\n",
        "        if val_map > best_val_map:\n",
        "            best_val_map = val_map\n",
        "            save_dict = {\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'epoch': t\n",
        "            }\n",
        "            torch.save(save_dict, model_name + '_best.pth')\n",
        "        save_dict = {\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'epoch': t\n",
        "        }\n",
        "        torch.save(save_dict, model_name + '_latest.pth')\n",
        "        train_history['loss'].append(train_loss)\n",
        "        train_history['loss_classifier'].append(train_loss_classifier)\n",
        "        train_history['loss_box_reg'].append(train_loss_box_reg)\n",
        "        train_history['loss_objectness'].append(train_loss_objectness)\n",
        "        train_history['loss_rpn_box_reg'].append(train_loss_rpn_box_reg)\n",
        "        val_history['map'].append(val_map)\n",
        "    print(\"Finished\")\n",
        "    return train_history, val_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjZss686a48Z"
      },
      "outputs": [],
      "source": [
        "# training and validation history\n",
        "\n",
        "train_history = {\n",
        "    'loss': [],\n",
        "    'loss_classifier': [],\n",
        "    'loss_box_reg': [],\n",
        "    'loss_objectness': [],\n",
        "    'loss_rpn_box_reg': []\n",
        "}\n",
        "val_history = {'map': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aPVQyTva48a"
      },
      "outputs": [],
      "source": [
        "# model layers\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "head_layers = ['roi_heads.box_predictor']\n",
        "rpn_layers = [layer[0] for layer in list(model.named_parameters()) if 'rpn' in layer[0]]\n",
        "backbone_layers = [layer[0] for layer in list(model.named_parameters()) if 'backbone' in layer[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fq9j0mAa48b"
      },
      "outputs": [],
      "source": [
        "def unfreeze_layers(model, layers_to_unfreeze):\n",
        "    '''\n",
        "    Unfreeze the specified layers in the model\n",
        "    '''\n",
        "    for name, param in model.named_parameters():\n",
        "        if any(layer in name for layer in layers_to_unfreeze):\n",
        "            param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvWlex7ba48b"
      },
      "outputs": [],
      "source": [
        "# train head\n",
        "\n",
        "if train_only_head:\n",
        "    unfreeze_layers(model, head_layers)\n",
        "    optimizer = torch.optim.Adam(model.roi_heads.box_predictor.parameters(), lr=1e-3)\n",
        "    num_epochs = 7\n",
        "    train_history, val_history = train(\n",
        "                                    model,\n",
        "                                    'faster',\n",
        "                                    num_epochs,\n",
        "                                    train_dataloader,\n",
        "                                    valid_dataloader,\n",
        "                                    optimizer,\n",
        "                                    train_history,\n",
        "                                    val_history\n",
        "                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9iaXJ-ta48c"
      },
      "outputs": [],
      "source": [
        "# train head + RPN\n",
        "\n",
        "if train_head_and_rpn:\n",
        "    model = faster_rcnn.to(device)\n",
        "    checkpoint = torch.load('faster_best.pth')\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    del checkpoint\n",
        "    unfreeze_layers(model, head_layers)\n",
        "    unfreeze_layers(model, rpn_layers)\n",
        "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, momentum=0.9, weight_decay=0.0005)\n",
        "    num_epochs = 3\n",
        "    train_history, val_history = train(\n",
        "                                model,\n",
        "                                'faster',\n",
        "                                num_epochs,\n",
        "                                train_dataloader,\n",
        "                                valid_dataloader,\n",
        "                                optimizer,\n",
        "                                train_history,\n",
        "                                val_history\n",
        "                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgR-a45ma48c"
      },
      "outputs": [],
      "source": [
        "# train head + RPN + backbone\n",
        "\n",
        "if train_all:\n",
        "    torch.cuda.empty_cache()\n",
        "    model = faster_rcnn.to(device)\n",
        "    checkpoint = torch.load('faster_best.pth')\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    del checkpoint\n",
        "    unfreeze_layers(model, head_layers)\n",
        "    unfreeze_layers(model, rpn_layers)\n",
        "    unfreeze_layers(model, backbone_layers)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=0.0005)\n",
        "    num_epochs = 20\n",
        "    train_history, val_history = train(\n",
        "                                model,\n",
        "                                'faster',\n",
        "                                num_epochs,\n",
        "                                train_dataloader,\n",
        "                                valid_dataloader,\n",
        "                                optimizer,\n",
        "                                train_history,\n",
        "                                val_history\n",
        "                                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clzl_7Aha48d"
      },
      "source": [
        "#### Training evolution analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V87CCXra48d"
      },
      "outputs": [],
      "source": [
        "def plotTrainingLoss(train_history):\n",
        "    '''\n",
        "    Plot the training loss history of the model\n",
        "    '''\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(train_history['loss'], label='Training Loss')\n",
        "    plt.plot(train_history['loss_classifier'], label='Classifier Loss')\n",
        "    plt.plot(train_history['loss_box_reg'], label='Box Regression Loss')\n",
        "    plt.plot(train_history['loss_objectness'], label='Objectness Loss')\n",
        "    plt.plot(train_history['loss_rpn_box_reg'], label='RPN Box Regression Loss')\n",
        "    plt.title('Training Loss History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jran2I5Aa48d"
      },
      "outputs": [],
      "source": [
        "# visualize training history\n",
        "\n",
        "plotTrainingLoss(train_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4A0izCwa48e"
      },
      "outputs": [],
      "source": [
        "def plotValidationMAP(val_history):\n",
        "    '''\n",
        "    Plot the validation mAP history of the model\n",
        "    '''\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(val_history['map'], label='Validation mAP')\n",
        "    plt.title('Validation mAP History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('mAP')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwnFvfGSa48e"
      },
      "outputs": [],
      "source": [
        "# visualize validation history\n",
        "\n",
        "plotValidationMAP(val_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VIqy5VNa48e"
      },
      "source": [
        "#### Model testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iay0s4v1a48f"
      },
      "outputs": [],
      "source": [
        "# load best model\n",
        "\n",
        "model = faster_rcnn.to(device)\n",
        "checkpoint = torch.load('faster_best.pth')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "del checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa1JZ1QRa48f"
      },
      "outputs": [],
      "source": [
        "# evaluate model on test data\n",
        "\n",
        "test_metrics = epoch_iter(test_dataloader, model, is_train=False)\n",
        "test_map = test_metrics['avg_map']\n",
        "print(f\"\\nTest mAP: {test_map:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMcovKJQa48g"
      },
      "outputs": [],
      "source": [
        "def show_predictions(model, dataloader, num_images=10):\n",
        "    '''\n",
        "    Display ground truth vs prediction in images\n",
        "    '''\n",
        "    model.eval()\n",
        "    image_count = 0\n",
        "    for batch, (images, targets) in enumerate(dataloader):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        with torch.no_grad():\n",
        "            predictions = model(images)\n",
        "            for i in range(len(images)):\n",
        "                fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "                axes[0].imshow(transforms.ToPILImage()(images[i]))\n",
        "                axes[0].set_title('True')\n",
        "                axes[0].axis('off')\n",
        "                for box in targets[i]['boxes']:\n",
        "                    box = box.cpu().numpy()\n",
        "                    rect = patches.Rectangle(\n",
        "                        (box[0], box[1]),\n",
        "                        box[2] - box[0],\n",
        "                        box[3] - box[1],\n",
        "                        linewidth=2,\n",
        "                        edgecolor='g',\n",
        "                        facecolor='none'\n",
        "                    )\n",
        "                    axes[0].add_patch(rect)\n",
        "                axes[1].imshow(transforms.ToPILImage()(images[i]))\n",
        "                axes[1].set_title('Predicted')\n",
        "                axes[1].axis('off')\n",
        "                boxes, scores, labels = apply_nms(predictions[i])\n",
        "                for box, score, label in zip(boxes, scores, labels):\n",
        "                    if score > 0.5:\n",
        "                        box = box.cpu().numpy()\n",
        "                        rect = patches.Rectangle(\n",
        "                            (box[0], box[1]),\n",
        "                            box[2] - box[0],\n",
        "                            box[3] - box[1],\n",
        "                            linewidth=2,\n",
        "                            edgecolor='b',\n",
        "                            facecolor='none'\n",
        "                        )\n",
        "                        axes[1].add_patch(rect)\n",
        "                        axes[1].text(box[0], box[1], f'{score:.2f}', bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "                plt.show()\n",
        "                image_count += 1\n",
        "                if image_count >= num_images:\n",
        "                    break\n",
        "        if image_count >= num_images:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWD4Wa9Ka48g"
      },
      "outputs": [],
      "source": [
        "# view predictions\n",
        "\n",
        "show_predictions(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1_vLYG-a48h"
      },
      "outputs": [],
      "source": [
        "# delete model data\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7CXxSSEa48h"
      },
      "source": [
        "### YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXIKhauba48h"
      },
      "source": [
        "#### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA4nZTuPa48h"
      },
      "outputs": [],
      "source": [
        "# data folder\n",
        "\n",
        "data_folder = '/content/datasets' if in_colab() else 'datasets'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue4PyIUna48i"
      },
      "outputs": [],
      "source": [
        "def populate_folder(dataloader, folder):\n",
        "    '''\n",
        "    Populate a folder with images and labels\n",
        "    '''\n",
        "    for i, (images, targets) in enumerate(tqdm(dataloader)):\n",
        "        for j, image in enumerate(images):\n",
        "            img = F.to_pil_image(image)\n",
        "            img.save(os.path.join(folder, 'images', f'{i}_{j}.jpg'))\n",
        "            with open(os.path.join(folder, 'labels', f'{i}_{j}.txt'), 'w') as f:\n",
        "                for box in targets[j]['boxes']:\n",
        "                    x_min, y_min, x_max, y_max = box\n",
        "                    x_center = (x_min + x_max) / 2.0 / 640\n",
        "                    y_center = (y_min + y_max) / 2.0 / 640\n",
        "                    width = (x_max - x_min) / 640\n",
        "                    height = (y_max - y_min) / 640\n",
        "                    f.write(f'0 {x_center} {y_center} {width} {height}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCJlpTVUa48i",
        "outputId": "3f911871-7d64-453e-ae88-50775a394730"
      },
      "outputs": [],
      "source": [
        "# create folders for train, valid, and test sets\n",
        "\n",
        "os.makedirs(data_folder + '/train/images', exist_ok=True)\n",
        "os.makedirs(data_folder + '/train/labels', exist_ok=True)\n",
        "populate_folder(train_dataloader, data_folder + '/train')\n",
        "\n",
        "os.makedirs(data_folder + '/val/images', exist_ok=True)\n",
        "os.makedirs(data_folder + '/val/labels', exist_ok=True)\n",
        "populate_folder(valid_dataloader, data_folder + '/val')\n",
        "\n",
        "os.makedirs(data_folder + '/test/images', exist_ok=True)\n",
        "os.makedirs(data_folder + '/test/labels', exist_ok=True)\n",
        "populate_folder(test_dataloader, data_folder + '/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create data.yaml file\n",
        "\n",
        "data_yaml = None\n",
        "\n",
        "if in_colab():\n",
        "    data_yaml = \"\"\"\n",
        "    names:\n",
        "        0: Lego\n",
        "    nc: 1\n",
        "    train: /content/datasets/train/images\n",
        "    val: /content/datasets/val/images\n",
        "    test: /content/datasets/test/images\n",
        "    \"\"\"\n",
        "    with open('/content/data.yaml', 'w') as f:\n",
        "        f.write(data_yaml)\n",
        "else:\n",
        "    data_yaml = \"\"\"\n",
        "    names:\n",
        "        0: Lego\n",
        "    nc: 1\n",
        "    train: train/images\n",
        "    val: val/images\n",
        "    test: test/images\n",
        "    \"\"\"\n",
        "    with open('data.yaml', 'w') as f:\n",
        "        f.write(data_yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmdVcJ2ra48j"
      },
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS0sCaOWa48j",
        "outputId": "5c737442-3ca2-4bad-811e-8f326e2a3f12"
      },
      "outputs": [],
      "source": [
        "# phase 1 - initial training with the first 10 layers frozen\n",
        "\n",
        "if in_colab():\n",
        "    !yolo task=detect mode=train model=yolov8s.pt data='/content/data.yaml' epochs=10 imgsz=640 name=yoloV8SLego_phase1 batch=8 freeze=10\n",
        "else:\n",
        "    !yolo task=detect mode=train model=yolov8s.pt data='data.yaml' epochs=10 imgsz=640 name=yoloV8SLego_phase1 batch=8 freeze=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL0DXBmda48j",
        "outputId": "2b0b7e92-a20f-496c-fb71-897b933842e3"
      },
      "outputs": [],
      "source": [
        "# phase 2 - unfreeze 5 layers and continue training\n",
        "\n",
        "if in_colab():\n",
        "    !yolo task=detect mode=train model=runs/detect/yoloV8SLego_phase1/weights/last.pt data='/content/data.yaml' epochs=10 imgsz=640 name=yoloV8SLego_phase2 batch=8 freeze=5\n",
        "else:\n",
        "    !yolo task=detect mode=train model=runs/detect/yoloV8SLego_phase1/weights/last.pt data='data.yaml' epochs=10 imgsz=640 name=yoloV8SLego_phase2 batch=8 freeze=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3H0YRX8a48k",
        "outputId": "6c238a5a-30a7-432d-d2dd-228ea18507ea"
      },
      "outputs": [],
      "source": [
        "# phase 3 - unfreeze all layers and finalize training\n",
        "\n",
        "if in_colab():\n",
        "    !yolo task=detect mode=train model=runs/detect/yoloV8SLego_phase2/weights/last.pt data='/content/data.yaml' epochs=10 imgsz=640 name=yoloV8SLego_phase3 batch=8 freeze=0\n",
        "else:\n",
        "    !yolo task=detect mode=train model=runs/detect/yoloV8SLego_phase2/weights/last.pt data='data.yaml' epochs=10 imgsz=640 name=yoloV8SLego_phase3 batch=8 freeze=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utIvroi5hj82"
      },
      "source": [
        "#### Training evolution analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "hgxX7I76hnqG",
        "outputId": "86f0173c-8d92-4bcd-8942-e27717b4ae24"
      },
      "outputs": [],
      "source": [
        "# visualize training and validation history - phase 1\n",
        "\n",
        "history = '/content/runs/detect/yoloV8SLego_phase1/results.png' if in_colab() else 'runs/detect/yoloV8SLego_phase1/results.png'\n",
        "img = cv2.imread(history)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Sx7kh72_kpRh",
        "outputId": "73516f02-97c8-424c-8a9a-127c7d69d596"
      },
      "outputs": [],
      "source": [
        "# visualize training and validation history - phase 2\n",
        "\n",
        "history = '/content/runs/detect/yoloV8SLego_phase2/results.png' if in_colab() else 'runs/detect/yoloV8SLego_phase2/results.png'\n",
        "img = cv2.imread(history)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "t2gYF_vCks8a",
        "outputId": "1823b45e-3e48-46cc-e28d-d2159a59d340"
      },
      "outputs": [],
      "source": [
        "# visualize training and validation history - phase 3\n",
        "\n",
        "history = '/content/runs/detect/yoloV8SLego_phase3/results.png' if in_colab() else 'runs/detect/yoloV8SLego_phase3/results.png'\n",
        "img = cv2.imread(history)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyLqUDe1jL-y"
      },
      "source": [
        "#### Model testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk7jlXH7jNsd"
      },
      "outputs": [],
      "source": [
        "# update data.yaml file - use test images\n",
        "\n",
        "data_yaml = None\n",
        "\n",
        "if in_colab():\n",
        "    data_yaml = \"\"\"\n",
        "    names:\n",
        "        0: Lego\n",
        "    nc: 1\n",
        "    train: /content/datasets/train/images\n",
        "    val: /content/datasets/test/images\n",
        "    test: /content/datasets/val/images\n",
        "    \"\"\"\n",
        "    with open('/content/data.yaml', 'w') as f:\n",
        "        f.write(data_yaml)\n",
        "else:\n",
        "    data_yaml = \"\"\"\n",
        "    names:\n",
        "        0: Lego\n",
        "    nc: 1\n",
        "    train: train/images\n",
        "    val: test/images\n",
        "    test: val/images\n",
        "    \"\"\"\n",
        "    with open('data.yaml', 'w') as f:\n",
        "        f.write(data_yaml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKyGEwJojc_3",
        "outputId": "7edc1d95-6ce2-4fc4-faae-196f99c4f5e9"
      },
      "outputs": [],
      "source": [
        "# evaluation metrics\n",
        "\n",
        "if in_colab():\n",
        "    !yolo task=detect mode=val model=runs/detect/yoloV8SLego_phase3/weights/best.pt name=yolov8Lego_eval data='/content/data.yaml' imgsz=640\n",
        "else:\n",
        "    !yolo task=detect mode=val model=runs/detect/yoloV8SLego_phase3/weights/best.pt name=yolov8Lego_eval data='data.yaml' imgsz=640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beDKKldEjqIi",
        "outputId": "53f4b97e-28c9-4386-92c8-3526f131905a"
      },
      "outputs": [],
      "source": [
        "# predictions\n",
        "\n",
        "if in_colab():\n",
        "    !yolo task=detect mode=predict model=runs/detect/yoloV8SLego_phase3/weights/best.pt name=yolov8Lego_test source=\"/content/datasets/test/images\" imgsz=640\n",
        "else:\n",
        "    !yolo task=detect mode=predict model=runs/detect/yoloV8SLego_phase3/weights/best.pt name=yolov8Lego_test source=\"datasets/test/images\" imgsz=640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oyhZUGzDl_6t",
        "outputId": "cb01a594-9ecb-43af-d9d9-1941b3a35e9b"
      },
      "outputs": [],
      "source": [
        "# show some predictions\n",
        "\n",
        "folder_path = '/content/runs/detect/yolov8Lego_test' if in_colab() else 'runs/detect/yolov8Lego_test'\n",
        "file_list = os.listdir(folder_path)\n",
        "num_images_to_display = 10\n",
        "num_cols = 2\n",
        "num_rows = (num_images_to_display + num_cols - 1) // num_cols\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 5*num_rows))\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    image_path = os.path.join(folder_path, file_list[i])\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    row_idx = i // num_cols\n",
        "    col_idx = i % num_cols\n",
        "    axes[row_idx, col_idx].imshow(image_rgb)\n",
        "    axes[row_idx, col_idx].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbcbQuRfa48k"
      },
      "source": [
        "# LEGO Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFS_IEpUa48k",
        "outputId": "f80e3b5b-aeeb-477f-ad23-a96b19cda122"
      },
      "outputs": [],
      "source": [
        "# auto annotation - bounding boxes from trained model to segment images\n",
        "\n",
        "if in_colab():\n",
        "    auto_annotate(data=\"/content/datasets/test/images\", det_model='/content/runs/detect/yoloV8SLego_phase3/weights/best.pt', sam_model=\"sam_b.pt\")\n",
        "else:\n",
        "    auto_annotate(data=\"datasets/test/images\", det_model='runs/detect/yoloV8SLego_phase3/weights/best.pt', sam_model=\"sam_b.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0fEFUlYxsPTy",
        "outputId": "69b361b2-3ceb-4ef2-f97b-9ca2990fb873"
      },
      "outputs": [],
      "source": [
        "# visualize some images with segmentation\n",
        "\n",
        "images_folder = \"/content/datasets/test/images\" if in_colab() else \"datasets/test/images\"\n",
        "annotations_folder = \"/content/datasets/test/images_auto_annotate_labels\" if in_colab() else \"datasets/test/images_auto_annotate_labels\"\n",
        "image_files = os.listdir(images_folder)[:10]\n",
        "\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(images_folder, image_file)\n",
        "    image = Image.open(image_path)\n",
        "    annotation_file = os.path.splitext(image_file)[0] + \".txt\"\n",
        "    annotations_path = os.path.join(annotations_folder, annotation_file)\n",
        "    with open(annotations_path, 'r') as file:\n",
        "        annotations = file.readlines()\n",
        "    mask = np.zeros_like(image)\n",
        "    for line in annotations:\n",
        "        line = line.strip().split()\n",
        "        class_id = int(line[0])\n",
        "        points = [(float(line[i]), float(line[i+1])) for i in range(1, len(line), 2)]\n",
        "        points = [(int(p[0] * image.width), int(p[1] * image.height)) for p in points]\n",
        "        mask = cv2.fillPoly(mask, [np.array(points)], (0, 255, 0))\n",
        "    mask_pil = Image.fromarray(mask)\n",
        "    merged_image = Image.blend(image, mask_pil, alpha=0.5)\n",
        "    plt.imshow(merged_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
